% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}
%\usepackage{longtable}
%\usepackage{caption}
\usepackage{graphicx}
\usepackage{caption}
\DeclareCaptionType{copyrightbox}
\usepackage{subcaption}
\usepackage{url}
%\usepackage{float}
%\usepackage{times}
%\usepackage{multirow}
%\usepackage{listings}
%\usepackage{times}
%\usepackage{paralist}
\usepackage{epsfig}
%\usepackage{subfigure}
%\usepackage{longtable}
\usepackage{hyperref}   
\hypersetup{colorlinks=false,urlcolor=blue}
\hypersetup{pdfborder={0 0 0}}
%\usepackage{subfigure}
\usepackage{color}
\usepackage[numbers]{natbib}
\usepackage{breakurl}
%\usepackage{ifpdf}
%\usepackage{wrapfig}
%\usepackage{float}
%\usepackage{texdraw}
%\usepackage{epsf}
%\usepackage{array}
%\usepackage{cite}
%\usepackage{enumitem}
%\usepackage{verbatim}
%\usepackage{setspace}
%\sloppy
%\usepackage{geometry}
%\usepackage{listings}
\usepackage{epstopdf}
\usepackage{grffile}


\newif\ifdraft
\drafttrue
\ifdraft
\newcommand{\abhi}[1]{ {\textcolor{red} { ***Abhinav: #1 }}}
\else
\newcommand{\abhi}[1]{ {}}
\fi
\newcommand{\ty}{\texttt}


\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{XSEDE}{'13 San Diego, CA USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{mlRho -- The Anatomy of a Successful Campus Bridging Project}
% \titlenote{(Produces the permission block, and
%copyright information). For use with
%SIG-ALTERNATE.CLS. Supported by ACM.}}
%\subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{6} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
%Scientific Applications and Performance Tuning (SciAPT),\\
%                University Information Technology Services,\\ 
%                Indiana University, Bloomington, IN, 47408\\
\alignauthor
Abhinav Thota\\
%       \affaddr{Scientific Applications and Performance Tuning (SciAPT)},\\
%       \affaddr{               University Information Technology Services,}\\
       \affaddr{              Indiana University, Bloomington, IN, 47408}\\
       \email{athota@iu.edu}
% 2nd. author
\alignauthor
Scott Michael\\
%       \affaddr{Scientific Applications and Performance Tuning (SciAPT)},\\
%       \affaddr{               University Information Technology Services,}\\
       \affaddr{              Indiana University, Bloomington, IN, 47408}\\
       \email{scamicha@iu.edu}
% 3rd. author
\alignauthor 
Sen Xu\\
%       \affaddr{Scientific Applications and Performance Tuning (SciAPT)},\\
%       \affaddr{               University Information Technology Services,}\\
       \affaddr{              Indiana University, Bloomington, IN, 47408}\\
       \email{senxu@indiana.edu}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor 
Bernhard Haubold\\
%       \affaddr{Scientific Applications and Performance Tuning (SciAPT)},\\
%       \affaddr{               University Information Technology Services,}\\
       \affaddr{              Max-Planck Institute for Evolutionary Biology, Pl\"{o}n, Germany}\\
       \email{haubold@evolbio.mpg.de}
% 5th. author
\alignauthor 
Tom Doak\\
%       \affaddr{Scientific Applications and Performance Tuning (SciAPT)},\\
%       \affaddr{               University Information Technology Services,}\\
       \affaddr{              Indiana University, Bloomington, IN, 47408}\\
       \email{tdoak@indiana.edu}
% 6th. author
\alignauthor 
Robert Henschel\\
%       \affaddr{Scientific Applications and Performance Tuning (SciAPT)},\\
%       \affaddr{               University Information Technology Services,}\\
       \affaddr{              Indiana University, Bloomington, IN, 47408}\\
       \email{henschel@iu.edu}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
An increasing number of researchers are outgrowing their desktops and turning to supercomputers to run their simulations and do their calculations. But today's computational problems are bigger than what individual universities can handle. XSEDE is the first place researchers turn to when they outgrow their campus resources. XSEDE machines are far larger (at least by an order of magnitude) than what most universities offer. Transitioning from a campus resource to a far larger XSEDE resource is not always trivial. XSEDE has taken many steps to make this easier, including the Campus Bridging initiative, the Campus Champions program, Extended Collaborative Support (ECS) program and through education and outreach. 

In this paper, we dissect a computationally intensive biology project and share our insights that can help strengthen the programs mentioned above. We are a team of biologists and application support analysts (including a Campus Champion). We worked on a project to calculate population mutation and recombination rates of tens of genome profiles using the mlRho code. mlRho is a serial, open-source, genome analysis code. We estimated that we need 6.3 million SUs. Two of the most important places where the biologists needed help in transitioning to XSEDE are (i) preparing the proposal for 6.3 million SUs on XSEDE, (ii) scaling up the existing workflow to hundreds of cores and (iii) performance optimization. The Campus Bridging initiative makes all of these tasks easier by providing tools and a consistent software stack across centers.

Ideally, campus champions are can provide support on (i), (ii) and (iii), while ECS staff can do (ii) and (iii). But (i), (ii) and (iii) are often not part of a campus champion's regular job description. To someone writing an XSEDE proposal for the first time, a link to the guidelines and a few pointers may not always be enough for a successful application. We believe that there is a role for a {\bf CAMPUS BRIDGIST} to play in filling the gaps that existing programs leave. While Campus Champions and ECS personnel are perfectly positioned to fill these gaps, we want to bring this to the policy makers' attention.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{PERFORMANCE, EXPERIMENTATION}

\keywords{high-throughput, XSEDE, BigJob, pilot-job, genetics}

\section{Introduction}
As first defined by the National Science Foundation Advisory Committee for Cyberinfrastructure's task force on
campus bridging \cite{nsf2011}, and later expanded upon by \citeauthor{stewart2012}, campus bridging is:

\begin{quotation}
``...the seamlessly integrated use of cyberinfrastructure operated by a scientist or engineer
with other cyberinfrastructure on the scientist's campus, at other campuses, and at the regional, national,
and international levels as if they were proximate to the scientist, and when working within the context of a
Virtual Organization (VO) make the 'virtual' aspect of the organization irrelevant (or helpful) to the work of
the VO. \cite{stewart2012}''
\end{quotation}

In applying this definition of campus bridging to XSEDE, one of the biggest challenges for researchers moving
from campus resources to XSEDE resources is being able to scale-up their workflows so that they are efficient
and can achieve high throughput on the larger XSEDE machines. While \citeauthor{stewart2012} identify key use
cases where campus bridging tools can improve a researcher's experience using XSEDE resources, one aspect that
is not included in their analysis is the dramatic increase in complexity that is inherent in the computational
and data storage systems when a researcher moves from his workstation to an XSEDE resource. This scale up in
complexity is in many cases a daunting prospect for a researcher new to XSEDE who may have relatively little
computational experience. Even savvy users who have experience with campus clusters can encounter issues when
scaling up to XSEDE resources.

This challenge is distinct from the challenge of providing a canonical software stack (proposed via a ROCKS
Roll by \citeauthor{stewart2012}), but can be compounded by widely divergent software environments between
campus and XSEDE resources. Even when the operating environment is not very different on XSEDE resources
compared to campus resources, researchers can face challenges with the sheer scale and complexity of XSEDE
resources. In the situation described in this paper, campus bridging amounted to helping the researchers
redesign their experiments and scale up their workflows to move effectively from their local workstations, to IU's modest
sized cluster environment, and then on to use the much larger XSEDE machines at TACC. It should be
noted that, in general, the differences in software environment between the IU campus resources and the TACC
supercomputers were superficial, the real challenges lie in dealing with scaling up the application and
navigating the complexity of a much larger system.

In this paper we introduce the new role of {\bf CAMPUS BRIDGIST} to help address the challenges faced by
researchers when moving from their workstation to XSEDE resources. A {\bf CAMPUS BRIDGIST} is someone who
helps to bridge the gap between domain science and computer science and helps researchers who want to use
XSEDE resources move their applications from small scale campus resources to much larger XSEDE resources. The
{\bf CAMPUS BRIDGIST} is mostly a technologist who is able to apply many different approaches to scaling up
applications, but should also be familiar with the common challenges of the scientific domain and should be
able to communicate effectively about XSEDE resources with researchers who may have little or no HPC
experience. The {\bf CAMPUS BRIDGIST} role is distinct from the currently existing Campus Champion role and
the role filled by the Extended Collaborative Support Service (ECSS) team. As the name implies, the {\bf
  CAMPUS BRIDGIST} role bridges the gap between the Campus Champion role which is designed to provide
information, guidance, and facilitate gaining access to XSEDE resources and the ECSS role which is designed to
provide in depth analysis, insight and development for a particular scientific or engineering challenge. The
role of {\bf CAMPUS BRIDGIST} may, in fact, be filled by the same people who are currently Campus Champions or
ECSS team members, in many ways the functions of the {\bf CAMPUS BRIDGIST} role are an extension of those
already existing ECSS and Campus Champion functions. In this paper we discuss how, by filling the role of
{\bf CAMPUS BRIDGIST}, we were able to bridge the gap between many differently sized resources and to help
researchers in the field of population genomics use some of the largest XSEDE resources to conduct a research
program at a magnitude that was unprecedented.

Researchers in the Indiana University (IU) Department of Biology have been conducting studies into population
genomics and evolution for some time. A newly developed software program called \texttt{mlRho} \cite{MEC:MEC4482}, is
being used to study ecological and genetic parameters in different populations. This incarnation of the
\texttt{mlRho} software is a serial code. However, the investigation we have conducted with our XSEDE allocation was
embarrassingly parallel in nature, so the work for each species was able to be divided among many
computational processes. To manage hundreds of processes for each of the 40+ species of interest, we used the
BigJob pilot-job tool, which is currently available on many XSEDE resources. BigJob allowed us to distribute
the analyses for each species across hundreds to a few thousand processor cores depending on the genome sizes
of species. Thus far we have conducted ML recombination rate studies on a total of {\bf XX} individual
genomes. In addition to the use of the BigJob pilot tool we have done extensive performance analysis of the
\texttt{mlRho} code and, with biologist and computer scientists working in conjunction, have been able to improve the
runtime of the code by a factor of {\bf XX}.

The remainder of the paper is organized as follows: section \ref{sec:background} gives some insight into the
computational methodologies and principles of biology being explored by the \texttt{mlRho} code. Section
\ref{sec:resources} outlines our initial estimates for the computational resources necessary to accomplish our
research agenda and the steps that were taken to secure an XSEDE allocation. In section \ref{sec:optimization}
we describe how through tracing and profiling of the code we were able to successfully optimize it and
dramatically increase its performance. Section \ref{sec:results} details the current status of the research
program and some initial science results and in section \ref{sec:conclusion} we present some conclusions.


\section{Scientific Background}\label{sec:background}
Understanding how the amazing biodiversity on our planet has arisen is a biological question that has
fascinated humans for thousands of years. It is critical to investigate the fundamental ecological and genetic
parameters (e.g., population sizes, recombination rates) to disentangle the evolutionary process of the
existing organisms. The ecological and genetic parameters play important roles in creating opportunities for
increasing genetic diversity, population divergence, and speciation. Deploying \texttt{mlRho}, a software package which
generates a novel measure of linkage disequilibrium, on XSEDE resources has allowed us to study these
important population-genetic parameters in a broad assembly of eukaryotic species. This approach has been
particularly fruitful when applied to the newly available genome sequences now being produced by
next-generation sequencing methods.

Although there are several methods for determining recombination rates, they can be both time and resource
intensive. The \texttt{mlRho} software employs a novel analytic approach that uses a new metric called the zygosity
correlation coefficient, which is estimated using maximum likelihood (ML). This approach requires single individual
samples, but is extremely data intensive. Using the ML approach implemented in the \texttt{mlRho} software developed at
IU, and XSEDE computational resources, we have been able to examine the recombination rates of a plethora of
species with accuracy that was previously unachievable.

\subsection{Program Description}
\abhi{say something about the serial nature of the code}
The \texttt{mlRho} software estimates mutation, recombination, and sequencing error rates from genome
sequences~\cite{MEC:MEC4482}. The underlying data consists of assembled sequencing reads obtained from a
single diploid individual. Such data are collected, for example, for the 1000 human genome project. At each
sequenced position \texttt{mlRho} reads the number of \ty{A}s, \ty{C}s, \ty{G}s, and \ty{T}s from a file. This
produces a ``profile''. Given a mutation and error rate, \texttt{mlRho} computes two probabilities for each
profile: The probabilities of observing the profile given that the position is either mutated (heterozygous),
or not (homozygous). These probabilities depend on the mutation and error rates. By varying them,
\texttt{mlRho} finds the values that maximize the overall likelihood of the data.

While mutation and sequencing error affect individual genome positions, recombination uncouples the
evolutionary history of pairs of positions. This is observable as a decorrelation of the zygosity states of
pairs of positions. To estimate recombination, \texttt{mlRho} computes the probability of observing profile pairs
separated by, say, 1000 nucleotides. This is a function of the recombination rate and the single position
likelihoods.

\subsection{Linkage Disequilibrium and Recombination Rate}\label{sec:LD}
Linkage disequilibrium (LD), i.e., the non-random association of alleles at two or more loci, is an important
parameter for many areas of population genetics.  In recent years, there has been a growing interest in
measuring LD across a broad range of species, because a proper understanding of LD would greatly facilitate
identifying the genetic loci which underlie important phenotypic variation in natural populations, as well as
human diseases. More importantly, LD is a population-genetic parameter that can help ascertain recombination
rate, because recombination is the primary evolutionary force that breaks down LD among genetic loci. Although
a substantial body of research has been devoted to determining the evolutionary advantages of
recombination~\cite{resolving-paradox}, the forces that determine recombination rates remain poorly
understood. For example, we have little idea what determines the occurrence of recombination on a chromosome,
what impact local DNA polymorphism has on recombination processes, and how recombination rates change over
evolutionary time~\cite{stumpf}.

Conventional approaches to measuring LD and recombination rates use population-genetic surveys. These surveys
require hundreds of individuals and dozens of genetic loci. Using this method, the sampling variance
associated with conventional measures of LD, such as D (a measure of LD) and $r^2$ (the square of the
correlation coefficient), is huge. Due to this fact, a large number of loci are needed to obtain a reasonable
estimate of LD.  Recombination rates can also be investigated by pedigree analyses and crossing experiments,
but these methods cannot provide information on fine-scale recombination rates and are often difficult to
perform, even in model organisms, let alone non-model species. Thus, while we know these values for a few
species, there is very little comparative data to understand how these processes vary across many species.

\subsection{ A Novel Approach to Estimating LD and Recombination Rate}\label{sec:migration}
Whole genome sequences of diploid organisms include both alleles at every site of the genome (two copies of
each chromosome), which can be used to determine a number of very useful population-genetic parameters in the
evolution a species. With the rapid accumulation of whole genome sequences from a large number of species, a
maximum likelihood (ML) approach that capitalizes on these data has recently been developed by researchers at
IU to estimate LD and examine genomic recombination patterns \cite{Lynch01112008,MEC:MEC4482}.

The general idea behind this approach is that two allelic chromosomes had a common ancestor some time in the past (just as any two humans share a common ancestor at some point in the past), and at that time, were identical. Since that time, they have become increasingly different, due both to mutations changing their sequence, and to each chromosome recombining with other chromosomes in the population (the larger the population, the more other chromosomes there are to recombine with). If we catalog the similarities and differences between the two alleles, we can learn about this history of mutation and recombination. 

The ML approach uses this information to determine the zygosity correlation ($\Delta$) between all the pairs of sites that are separated by various distances (i.e., the probability of two sites being both homozygous or both heterozygous) in a single diploid genome, using the entire set of individual reads. Population genetic theory then links the expected value of delta to conventional measures of LD, such as D and $r^2$, suggesting that $\Delta$ can be used as a valid measure of LD on the population level.

% {\center \bf Insert equation 11, 14, and 5d from Lynch unpublished.}
% \begin{eqnarray}
% E(\Delta) \simeq {4E(D^2)\over \pi(1-\pi)}
% \label{eq:edelta}
% \end{eqnarray}

%  \begin{eqnarray}
% E(\Delta) = {{\theta(1+\theta)(18+\rho)[A+2-22\theta \rho(3+\rho+12\theta)]} \over {(10+\rho+8\theta)A}}r^2
% \label{eq:edelta}
% \end{eqnarray}

%  \begin{eqnarray}
% A = 9+6.5\rho + 0.5\rho^2+19\theta \rho+12\theta^2\rho+\theta \rho^2+54\theta+80\theta^2+32\theta^3
% \label{eq:edelta}
% \end{eqnarray}


% It has become clear that delta is a function of the population mutation rate ($\theta$= 4 Ne $\mu$) and population recombination rate ($\rho$ = 4Nr), from which theta and rho can be estimated.

% {\center \bf Insert equation 6 and equation 5d from Lynch unpublished.}


%  \begin{eqnarray}
% E(\Delta) \simeq {{\theta(1+2\theta)(18+\rho)} \over {2(1+\theta)A}}
% \label{eq:edelta}
% \end{eqnarray}

%  \begin{eqnarray}
% A = 9+6.5\rho + 0.5\rho^2+19\theta \rho+12\theta^2\rho+\theta \rho^2+54\theta+80\theta^2+32\theta^3
% \label{eq:edelta}
% \end{eqnarray}

% Thus, when rho is zero, $\Delta$ is approximated by $\theta$; when $\rho$ is infinite, $\Delta$ is approximated by u/r, a parameter that reflects the relative power of mutation versus recombination. These two parameters are critical for evaluating the mutational-hazard hypothesis for genome evolution (Lynch 2007)
Our simulation results have shown that this ML procedure generates unbiased estimates for theta and
zygosity-correlation at a range of sequencing coverage (10-20x) \cite{Lynch01112008}.  This ML technique has
been fully implemented in the software package \texttt{mlRho}. The \texttt{mlRho} software is a highly
versatile program, able to handle genomes of various sizes, and linkage analyses over hundreds of thousands of
base pairs. The maximum-likelihood estimates for $\Delta$ and recombination rates are efficiently sought using
the Nelder and Mead algorithm, as implemented by the GNU scientific library. Nonetheless, this analysis is
computationally extremely intensive: millions to tens of billions of pairs of sites at a given distance have
to be extracted from the input files (usually of giga-byte sizes) and maximum likelihood estimates of $\Delta$
have to be calculated for every distance.


\section{Transitioning to XSEDE}\label{sec:resources}
When we first came up with a research plan, we calculated that we need 12 million compute hours to complete the project without sacrificing any of the objectives. At this point, we were using Quarry, a local IU resource, but it was clear that we needed a bigger machine to complete the calculations in a reasonable period of time. Quarry is a 2960 core machine, where as XSEDE machines such as Ranger, Stampede and Kraken are much larger. Figure \ref{fig:scaling} shows the number of nodes and cores on each machine. The environment on Quarry is similar to that of XSEDE machines referred to here. The central difference is the size of the machines and therefore the biggest challenge is scaling our workflow accordingly. We explain how we made the transition and discuss the tools that we used to succeed on this mission.
%\begin{table}
%\centering
%\begin{tabular}{| l  | l  | l  | l  | l  | l  | l  |} \hline
%	&Quarry & Ranger & Stampede & Kraken\\ \hline
%Nodes & 370 & 3936 & 6400 & 9408   \\ \hline
%\# Cores	& 2960  & 62976&102400 & 112896\\ 
%\hline
%
%\end{tabular}
%\caption{\abhi{decide table vs plot}  } 
%\label{table:cache_comp}
%\end{table}


\begin{figure} %[h!] % t=top, b=bottom, h=here, p=separate page
\centering
\includegraphics[width=0.48\textwidth]{figures/cores-nodes.png}
\caption{The graph shows the huge change in the size of the machines that the users experience when they move to XSEDE. Mason and Quarry are IU resources, where as Ranger, Stampede and Kraken are XSEDE resources. }
\label{fig:scaling}
\end{figure}

\subsection{Research Plan}\label{sec:plan}

%\abhi{change the following paragraph to indicate what we actually did and why. i.e. doing more kbp/genome and may be more genomes}

Once we decided to transition to XSEDE resources, we applied for a startup allocation on Ranger and Kraken to benchmark our application and do a scaling study to accurately estimate the number of SUs we would need. It was immediately apparent to us that we would need at least 15 million SUs to complete the project without leaving out any of the genomes. But given that the researchers are new to the XSEDE ecology and this is the first time they are applying for a large scale allocation, we wanted to stay about the 5 million SU mark. To do this, we pared down the list of to 46 diploid eukaryotic genomes, covering a broad range of genome sizes.
The 100 kbp cutoff value is chosen because theory suggests that a good measure
of LD in a genome should span at least 50$-$100 kbp. Given that in eukaryotes, recombination rates range
between 0.001$-$1 event per Mbp~\cite{annurev-genom-082410-101412}, a 100 kbp window across a genome provides
a good basis for capturing the signature of crossover events that occur 1 or 2 times per chromosome arm per
meiotic event.

Our approach to linkage analyses requires diploid genomes that have not been intentionally inbred  (e.g., some plant genomes) or sequenced from several individuals. The list of 46 species are carefully selected from several important taxonomic groups to represent organisms with different ecology, population history, and reproductive strategies  (e.g., asexual vs. sexual). This list  thus constitutes a core dataset to develop a broad survey of linkage disequilibrium, population recombination rate, and population histories across eukaryotes. Furthermore, we have included genomes from multiple individuals for both {\it Daphnia pulex} and humans to explore intraspecific differences in linkage patterns and population histories. 

\subsection{Scaling Up to XSEDE}
\label{sec:tests}

The researchers run the mlRho code on local IU resources serially, by requesting one node at a time. Given that IU resource Quarry had a serial queue, which placed multiple serial jobs from a single user on the same node, this was a feasible course. But as far as we know, XSEDE resources do not provide such a queue. The users are charged for the whole node, irrespective of the number of cores the user actually utilizes. Moreover, the larger machines are configured for highly parallel jobs and the schedulers are configured to prioritize larger jobs. Many XSEDE centers provide workarounds for this problem by providing users with wrappers and other software which bundle many serial jobs into larger jobs. 


\subsubsection{SAGA BigJob framework}
\label{sec:bigjob}

We used the SAGA BigJob to bundle our single-core serial mlRho simulations. We have integrated mlRho with BigJob and conducted several experiments on Ranger and Stampede which have shown good performance and scalability. Here we introduce BigJob.

BigJob is pilot-job tool available on many XSEDE resources such as Kraken, Ranger and Lonestar. Many researchers have successfully used BigJob~\cite{bigjob_web} to bundle hundreds of smaller jobs into larger, more manageable groups of jobs~\cite{Luckow:2008fp, async_repex11}. 

BigJob allows the user to maintain a list of processors allocated after the job request becomes active. The user can then assign these processors and start and manage jobs. For more details on the BigJob implementation see reference~\cite{saga_bigjob_condor_cloud}. The main benefit of doing this is that instead of submitting thousands of single core job requests to the queue, we can submit hundreds of large job requests ($\approx$ 500 to 5000 cores) to the queue. This reduces the overall job submissions to the queue and thereby, to an extent, time spent waiting in the queue. This job size is also more appropriate for many of the XSEDE machines. The experiments are explained in the next section. 

\begin{table}
\centering
\begin{tabular}{|c|c| p{1.3cm} |c  |     } \hline
Organism Type		& Size of profile& \multicolumn{2}{|c|}{Distanc/sec}  \\ \hline
	&   & V 1.10 & V 2.1 \\ \hline
{\it F. cylindrus}  & 0.72 GB & 0.0034 & 0.323 \\ \hline
{\it P. ornithorhynchus}  & 11 GB &$2.3{\times}10{-4}$ & 0.020 \\ \hline
{\it C. familiaris}  & 31 GB & $8.1{\times}10^{-5}$ & 0.005 \\
\hline
%Intel & 405 & 388 & 364 & 346\\
%\hline
%PGI &418 & N/A   & N/A &N/A\\
%\hline

\end{tabular}
\caption{The table lists three organisms which were chosen to represent profiles of different size. The second column shows the size of the profile in gigabytes. The third column shows the distance travelled per second by V1.10 and V2.1 on one core on Ranger and Stampede, respectively. The rate of distance is a function of the size of the genome.  } 
\label{table:cache_comp}
\end{table}

\subsubsection{mlRho Scalability tests}
\label{sec:scalability}
We used the BigJob installation already in place on Ranger. The major focus of our tests was to find out how mlRho scales when we increase the number of concurrent mlRho processes. 

We took three different organisms based on the size of their genome. We selected three sizes -- small (0.72 GB), medium (11 GB) and large (31 GB). We ran the mlRho program with each of these genomes. We started with 16 concurrent mlRho instances which are reading from the same data file, and let it run for 24 hours. We repeated this for each of these genomes with 16, 32, 64 and 128 concurrent instances. The results are shown in figure~\ref{fig:bj-scaling}. Figure~\ref{fig:bj-scaling} shows that the distance traveled, irrespective of the genome, size and the number of concurrent instances, increases linearly with number of concurrent instances being run. We have verified this behavior on Stampede with an optimized version of mlRho with similar results. We are unable to repeat this on Ranger as it is in the process of getting decommissioned. We discuss how we analyzed the performance and optimized the code in the next section. 

%Table~\ref{table:bj_runs} shows the distance traveled by each instance in 24 hours for different genomes. This also shows that the distance traveled is a function of the size of the genome data file. As the reader can observe, distance traveled is directly proportional to the size of the genome data file. We were able to estimate the SUs required per distance per GB of genome data file using each of the three genomes -- all three estimates are nearly identical. Based on these results, we are confident that we can scale up to more than 1000 concurrent instances per genome using BigJob.
\begin{figure*}[t] %[h!] % t=top, b=bottom, h=here, p=separate page
\centering
\includegraphics[width=0.78\textwidth]{figures/bj-scaling.png}
\caption{After running 16, 32, 64 and 128 instances of mlRho concurrently using BigJob on Ranger and Stampede with versions 1.10 and 2.1, respectively, we see that amount of work done increases nearly linearly with the number instances of mlRho. The y-axis shows the total distance travelled by all the instances put together and normalized to 16 instances. The x-axis shows the number of concurrent mlRho processes running. }
\label{fig:stampede-bench}
\end{figure*}

%\begin{figure*}[htp]
%  \centering
%
%
%  \subfloat[Scaling characteristics of version 1.10 on Ranger ]{\label{fig:ranger}\includegraphics[width=80mm]{figures/scaling-ranger.png}}
%  \subfloat[Scaling characteristics of version 2.1 on Stampede]{\label{fig:stampede}\includegraphics[width=80mm]{figures/scaling-stampede.png}}
%  \label{fig:bj-scaling}\caption{After running 16, 32, 64 and 128 instances of mlRho concurrently using BigJob on Ranger and Stampede with versions 1.10 and 2.1, respectively, we see that amount of work done increases linearly with the number instances of mlRho. The Y-Axis shows the total distance travelled by all the instances put together and normalized to 16 instances. The X-Axis shows the number of concurrent mlRho processes running. }
%\end{figure*}

\subsection{XSEDE Allocation}
%\abhi{notes: startup alloc help -- CBridgers's help needed, scalability tests, etc}

Obtaining an XSEDE allocation with sufficient SUs is an important step in the process of executing the research plan. To someone who does have a computer science background, writing a successful XSEDE proposal is not a trivial task. The computational justification for the allocation needs to be concrete and this is especially true if the request is for more than a few hundred thousand SUs. XSEDE allocation committees are routinely faced with the fact that the machines are oversubscribed by a ratio (\abhi{??}) on average. 

We believe that this is the part where users new to XSEDE need significant help. In this case, research staff from PTI helped the biologists to prepare an allocation proposal. We started by requesting startup allocations on Ranger and Kraken. We benchmarked mlRho on a single core on Ranger and Kraken. We then did a scalability study as described in section \ref{sec:scalability}. It is important to accurately estimate and justify the total number of SUs that we request from XSEDE.  We also need to justify the science, but if the research is supported by a grant, further justification is not needed. Given that we are requesting shared resources, it is important that we make an effort to analyze and optimize our code. This reduces our own usage time and also turnaround time. We started looking into this aspect at the time of submitting the proposal. This is discussed in section \ref{sec:optimization} in more detail. 


\subsubsection{Design of Experiments}

We have carefully constructed a detailed plan on how to proceed with our experiments. In section \ref{sec:tests}, we have shown that we can scale to 128 processes reading from the same file with no performance degradation. We estimate that we can feasibly scale-up to $\approx$500 processes reading from the same data file, without noticeable performance degradation. By making multiple copies of the data file, we could scale-up to $\approx$ 5000 processes. Job requests of 5000 processes on Ranger and Kraken are appropriate and we have not experienced extremely long wait times with past BigJob experiments. If we succeed in implementing this design, we think we can complete our analysis within four to five months from the time of the allocation award. This proposed framework is already working on Ranger, and we are in the process of implementing it on Kraken. 

In our initial testing of the three sample genomes, we worked out a few issues that could have affected
scalability. One of those issues was data access and file striping. When we had scaled up to 128 cores we
noticed intermittent I/O issues. Since all the concurrent instances read from a single file, if that file is
singly striped in a Lustre file system it will introduce a large load on the Lustre servers. We have remedied
this issue by moving the data files to the scratch file system on Ranger and striping the data files 16
ways. While this only occurred with the larger (11 and 31 GB) data files, we are aware that this might limit
our scalability. In the future, if I/O becomes an issue for scalability, we plan to use multiple copies of the
same data file to prevent an excessive number of concurrent processes from reading from the same file at the same time.

\section{Optimization and Performance Tuning of mlRho}\label{sec:optimization}
\subsection{Performance Analysis}\label{subsec:analysis}
%\subsection{Source-code Development and Optimization}
The \texttt{mlRho} application was originally developed through a collaboration of research labs at IU and the
Max Planck Institute for Evolutionary Biology in Germany. Following initial benchmarking and scalability
testing for an XSEDE allocation proposal, staff members from the Pervasive Technology Institute at IU worked
together with the \texttt{mlRho} application developers at IU and the Max Planck Institute for Evolutionary
Biology to improve the serial performance of the code. As a result of the investigations by PTI, a new version
has been recently released, which is vastly more efficient and delivers much more performance when compared to
the original version. While the work described in the previous sections could be addressed by the {\bf CAMPUS
  BRIDGIST} role, the analysis and optimization described in this section would most likely be associated with
the XSEDE ECSS team, in this particular case both roles were fulfilled by a single team member at PTI, however
it is certainly possible that these two roles could be filled by different teams at different institutions.

\subsection{Implementation of  Performance Analysis Findings}
Research staff at PTI began code optimization with \texttt{mlrho} version 1.10. As a first step we compiled
the code with compilers other than the standard GNU compiler that had been used by \texttt{mlrho}. Both the
Intel and PGI compilers produced a runtime improvement of $\approx$ 10\% over the GNU compiler on Ranger, this
is a fairly typical result that the optimization team at PTI has seen in many instances. From this point we
determined that further improvements would most likely be gained by modifications to the source code. To
inform the core developers at the IU and Max Planck Institute for Evolutionary Biology research labs as to
where their efforts would be best spent, we conducted a detailed analysis of \texttt{mlrho} version 1.10 using
the Vampir toolchain. The analysis led the core developers to focus on two aspects of the code: data handling
and repeated computation. As to data handling, many profiles occur repeatedly in a data set. To address this
an additional program, \ty{formatPro}, was written to compress the raw profiles. The \ty{formatPro} program reads profiles either from a text
file, or from a BAM file, the  standard format for distributing genome alignment files \citep{li09:seq}. 

By introducing this new method of data storage performance increased by more than a factor of two from version
1.16 to 1.21 (see figure \ref{fig:stampede-bench}). The \ty{formatPro} program writes a binary table of unique
profiles.
% \begin{lstlisting}
% /* write tag for recognizing file type */
% fwrite("sum",sizeof(char),3,profileF);
% /* note the number of profiles */
% fwrite(&numNode,sizeof(int),1,profileF);
% /* write the profiles */
% fwrite(profiles,sizeof(Profile),numNode,profileF);
% \end{lstlisting}
%where a profile is
% \begin{lstlisting}
% typedef struct profile{  
%   int profile[4];        /* profile               */
%   int n;                 /* number of occurrences */
% }Profile;
% \end{lstlisting}
The binary file can then be inspected using the program \ty{inspectPro}.
% \begin{verbatim}
% inspectPro profileDb.sum | head -n 3
% #ID	Count	A	C	G	T
% 0	1411292	0	0	4	0
% 1	1410182	4	0	0	0
% \end{verbatim}
In addition to the profiles file, \ty{formatPro} writes a binary file
listing the profile ID at every genome position.
% \begin{verbatim}
% inspectPro profileDb.pos | head -n 3
% #Pos	Pro
% 1	0
% 2	1
% \end{verbatim}
Finally, \ty{formatPro} writes a binary file of the contig lengths.
% \begin{verbatim}
% inspectPro profileDb.con | head -n 3
% #ID	Length
% 0	434
% 1	32
% \end{verbatim}
The \ty{mlRho} program can then read the profiles from the files produced by \ty{formatPro}.
% \begin{lstlisting}
% /* read first three characters */
% fread(&tag,sizeof(char),3,fp);
% /* read the number of profiles */
% fread(&numProfiles,sizeof(int),1,fp);
% /* allocate space for profiles */
% profiles = (Profile *)malloc(numProfiles*sizeof(Profile));
% /* read profiles */
% for(i=0;i<numProfiles;i++)
%   fread(&profiles[i],sop,1,fp);
% /* make profiles available for further computations */
% setProfiles(profiles);
% setNumProfiles(numProfiles);
% \end{lstlisting}
The profiles are read individually rather than in a single
step, because we found that this was
faster. The improvement in data handling introduced with these changes resulted in an overall speedup of the
serial code of a factor of 2-4X. The next focus was to look at the repeated computation that occurred in the
\ty{mlRho} program. 

In the 1.10 version of the \ty{mlRho} program, the likelihood computation iterated over all positions. By
introducing several improvements in how the likelihoods are calculated, the program now iterates over the much
smaller number of unique profiles, this improvement can be noticed in figure \ref{fig:stampede-bench} between
versions 1.21 and 1.23.
% \begin{lstlisting}
% for(i=0;i<numProfiles;i++){
%   /* likelihood given that position is homozygous */
%   lOneLoc = lOne(coverages[i],profiles[i].profile,ee);
%   /* likelihood given that position is heterozygous */
%   lTwoLoc = lTwo(coverages[i],profiles[i].profile,ee);
%   l = lOneLoc * (1.0 - pi) + lTwoLoc * pi;
%   likelihood += log(l) * profiles[i].n;
% }
%\end{lstlisting}
The calculation of the unique likelihoods are now written to disk for future reference.
% \begin{lstlisting}
% /* write tag for identifying file */
% fwrite("lik",sizeof(char),3,fp);
% /* write size of data structure */
% fwrite(result,sizeof(Result),1,fp);
% /* save number of profiles considered */
% np = getNumProfiles();
% fwrite(&np,sizeof(double),1,fp);
% /* write likelihoods given homozygosity */
% fwrite(getLones(),sizeof(double),getNumProfiles(),fp);
% /* write likelihoods given heterozygosity */
% fwrite(getLtwos(),sizeof(double),getNumProfiles(),fp);
% \end{lstlisting}

In the disequilibrium analysis of the 1.10 version, the single-site likelihoods were recomputed for every
step. To improve efficiency they are now either read from disk or stored after the first pass across the
data. In addition, we noticed that by ignoring the order of two profiles, we could halve the number of
distinct profile pairs stored in a search tree. The likelihood computation during traversal of this tree is
now based entirely on precomputed probabilities.
% \begin{lstlisting}
% void traverse(int a, Node *np, double h0, double h2, double complementHalf){
%   if(np != NULL){
%     traverse(a,np->left,h0,h2,complementHalf);

%     b = np->key;
%     li = h0*lOnes[a]*lOnes[b]
%        + h2*lTwos[a]*lTwos[b]
%        + complementHalf*(lOnes[a]*lTwos[b]+lTwos[a]*lOnes[b]);
%     likelihood += log(li) * np->n;

%     traverse(a,np->right,h0,h2,complementHalf);
%   }
% }
% \end{lstlisting}
% where \ty{h0} is the probability of observing a homozygous pair and
% \ty{h2} the probability of observing a heterozygous pair. These two
% quantities are a function of the rate of recombination. The variable
% \ty{complementHalf} simply stores $1-(\ty{h0} + \ty{h2})$.
The combination of better data handling and careful avoidance of repeated computation, lead to an overall 60-fold
speedup of \ty{mlRho} while maintaining its minimal memory requirement.

\begin{figure} %[h!] % t=top, b=bottom, h=here, p=separate page
\centering
\includegraphics[width=0.48\textwidth]{figures/mlrho-benchmarks.png}
\caption{The graph shows the different versions of mlRho benchmarked on a single core on Stampede. The y-axis shows the time steps per hour in logarithmic scale. Compared to version 1.10,  2.1 is nearly 50 times faster. }
\label{fig:stampede-bench}
\end{figure}

%\begin{table}
%\centering
%\begin{tabular}{| l  | l  | l  | l  | l  | l  | l  |} \hline
%	&Ranger & \multicolumn{3}{|c|}{Mason}\\ \hline
%	& 1.11  & 1.10&1.11 & 1.14\\ \hline
%GNU & 450 & 508 & 414 & 394   \\
%\hline
%Intel & 405 & 388 & 364 & 346\\
%\hline
%PGI &418 & N/A   & N/A &N/A\\
%\hline
%
%\end{tabular}
%\caption{\abhi{placeholder}The table shoes mlRho runtimes on different machines with different compilers. The runtimes are in seconds. Compiling with the Intel compiler gave us a 10\% speedup compared to GNU/PGI compilers on Mason and Ranger. Future work could include building GSL with Intel compilers.   } 
%\label{table:cache_comp}
%\end{table}

% \subsection{Code Validation}
% To determine the accuracy of the revamped \ty{mlRho}, we simulated sequencing data with known mutation and
% recombination rates. Figure~\ref{fig:test}A shows that the mutation rate is accurately estimated even if
% exceeded by the error rate. Similarly, Figure~\ref{fig:test}B shows that the recombination rate is slightly
% underestimated if the mutation rate $\theta=10^{-2}$ and $\rho=10^{-2}$ or $\rho=10^{-3}$. However, the
% combination of low mutation and recombination ($\theta=\rho=10^{-3}$) gives very noisy results, demonstrating
% the method's limit of resolution.

% \begin{figure*}
%   \begin{center}
% \unitlength1in
% \begin{minipage}{0.45\linewidth}
% \centering
% \includegraphics{figures/theta}
% \end{minipage}
% \begin{minipage}{0.45\linewidth}
% \centering
% \includegraphics{figures/rho}
% \end{minipage}

% \caption{Testing the accuracy of \ty{mlRho}. \textbf{A}: The mutation
%   rate estimator, $\hat\theta$, as a function of the sequencing error
%   rate, $\epsilon$; sequence length: $10^8$. \textbf{B}: The
%   recombination rate estimator, $\hat\rho$, as a function of pairwise
%   distance; the data set consists of 1000 pairs of 100kb sequences
%   with $\theta=0.01$ and $\epsilon=10^{-4}$; horizontal lines:
%   expected value. Coverage: 10.}\label{fig:test}
%   \end{center}
% \end{figure}

\section{User Experience, Preliminary Science Results, and Future Work}\label{sec:results}
\subsection{User Experience}
Overall the end user experience of migrating from the relatively modest Quarry and Mason local clusters to the
much larger Ranger resource on XSEDE was a very smooth transition. This was mostly due to two factors the
first of which was the similar computational environment on campus and XSEDE resources.. All the data files were easily transferred by standard means. The necessary modules and libraries are all available on Ranger and Stampede.
 
My experience with running mlRho analysis using Bigjob on Ranger consisted of two different phases in terms of the program performance. When we were developing this idea to use Bigjob to run the analysis in an embrassingly parallel way (prior to July 2012), the performance of Bigjob was excellent. All the multicore jobs were completed nicely. However, after the approval of our allocation in September 2012, Bigjob had been upgraded. There seems a number of issues with Bigjob. First of all, the way Bigjob is run involves using the command nohup. As all the jobs are submitted on the login node of Ranger, it is required that no more than 4 nohup commands can be run at one time. It occurred a couple times that I submitted more than 4 bigjobs on the login node and my jobs were all suspended. Subsequently, I was banned from running the jobs. Unfortunately, I was not notified until I found all my jobs were not running for two to three days. This seems to me rather counter productive. For my project, I need to submit ~100 multicore jobs (requiring on average 1024-4096 cores) in total. I would like to have the capacity to submit a number of jobs in the queue and collect data when the jobs are done. Assuming each job is done right, submitting four jobs at a time is not problematic. What is truly problematic is that I encountered a situation that many cores (up to 60\% of the cores) in the same job cannot finish the analysis properly, resulting many missing values. Finding out the missing values and creating new jobs will not necessarily solve the problem, because I often had cores not working properly in these new jobs.
 
The queuing time on Ranger was largely determined by how many jobs each user have used in the recent past. For a heavy user in the last one or two days, the queuing time might be 1-2 days for new jobs. The Stampede is excellent in terms of queuing time. At this moment, any job gets started almost immediately.

\subsection{Preliminary Science Results}
%note from Sen
The speeding up of mlRho program greatly enabled the expansion of this project. Instead of performing analysis to 100 kilo basepairs, we now can carry analyses up to 1 million basepairs.
In fact, the originally proposed work load has been completed. We basically did 10 times more than what we
planed to do.

\begin{figure} %[h!] % t=top, b=bottom, h=here, p=separate page
\centering
\includegraphics[width=0.48\textwidth]{figures/mic-scaling.pdf}
\caption{The graph compares the performance scaling behavior of mlRho on CPUs and MICs. Each mlRho process on a CPU was run on an individual core while multiple instances of mlRho were run on a single MIC. 488 mlRho instances on a single MIC gave us the same throughput as running 32 instances on 2 nodes of Stampede.  }
\label{fig:mic-scaling}
\end{figure}

\subsection{Future Work}
We have begun initial work on deploying the \ty{mlrho} program on the Intel Xeon Phi coprocessor boards
available on Stampede. Since the Phi runs an embedded Linux Operating System \cite{xeon_phi}
it is relatively easy to launch multiple copies of the \ty{mlrho} binary on the Phi board, assuming
that the input data set can fit in the memory footprint of the Phi board. In our case we copied input data
sets to the Phi RAM disk and computed against this copy. Figure \ref{fig:mic-scaling} compares the performance
of a single Phi board on the {\it F. cylindrus} genome with the scaling measurements presented in figure
\ref{fig:stampede-bench}. Here we compare to the scaling numbers for the Stampede timings using version 2.1 of
the \ty{mlrho} software. By using a relatively large number of processes, in this case 488, on the Phi board
we have been able to achieve throughput that is roughly equal to the throughput of the CPUs on two Stampede
nodes. We are currently focusing our efforts on integrating the Phi scripts into the BigJob framework and
adding the Phi workload to the CPU workload. Writing a script for the BigJob framework is fairly
straightforward, the challenge comes in properly balancing the load between CPU and Phi, particularly when the
input data access patterns (i.e. the input I/O) is very different for the CPUs and the Phi board.


\section{Conclusions}\label{sec:conclusion}

In all, the project of transitioning and scaling up \ty{mlRho} workloads from campus computational resources
to XSEDE resources has been very successful, not only from the perspective of accelerating scientific
discovery, but also from the perspective of providing a very real and useful example of the value of a {\bf
  CAMPUS BRIDGIST}. Through this project we have shown that a relatively small investment of effort by people
with the right mix of skills can make a big difference in how challenging it is to transition from local
resources to XSEDE resources. We propose that XSEDE consider including the {\bf CAMPUS BRIDGIST} role in more
of its supported projects, particularly those projects with PIs who are relatively new to high performance
computing concepts like batch scheduling, application scalability, and high performance file systems. As the
\ty{mlrho} software continues to be improved and applied to more data sets, we hope to continue the excellent
synergistic relationship between domain scientists, computer scientists, and cyberinfrastructure
professionals.


%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrvnat}
\bibliography{xsede13}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
\end{document}
