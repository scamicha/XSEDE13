% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}
%\usepackage{longtable}
%\usepackage{caption}
\usepackage{graphicx}
%\usepackage{url}
%\usepackage{float}
%\usepackage{times}
%\usepackage{multirow}
%\usepackage{listings}
%\usepackage{times}
%\usepackage{paralist}
%\usepackage{epsfig}
%\usepackage{subfigure}
%\usepackage{longtable}
%%\usepackage[hypertex]{hyperref}
%\usepackage{subfigure}
\usepackage{color}
\usepackage[numbers]{natbib}
%\usepackage{ifpdf}
%\usepackage{wrapfig}
%\usepackage{float}
%\usepackage{texdraw}
%\usepackage{epsf}
%\usepackage{array}
%\usepackage{cite}
%\usepackage{enumitem}
%\usepackage{verbatim}
%\usepackage{setspace}
%\sloppy
%\usepackage{geometry}


\newif\ifdraft
\drafttrue
\ifdraft
\newcommand{\abhi}[1]{ {\textcolor{red} { ***Abhinav: #1 }}}
\else
\newcommand{\abhi}[1]{ {}}
\fi



\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{XSEDE}{'13 San Diego, CA USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{mlRho -- The Anatomy of a Successful Campus Bridging Project}
% \titlenote{(Produces the permission block, and
%copyright information). For use with
%SIG-ALTERNATE.CLS. Supported by ACM.}}
%\subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{6} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
%Scientific Applications and Performance Tuning (SciAPT),\\
%                University Information Technology Services,\\ 
%                Indiana University, Bloomington, IN, 47408\\
\alignauthor
Abhinav Thota\\
%       \affaddr{Scientific Applications and Performance Tuning (SciAPT)},\\
%       \affaddr{               University Information Technology Services,}\\
       \affaddr{              Indiana University, Bloomington, IN, 47408}\\
       \email{athota@iu.edu}
% 2nd. author
\alignauthor
Scott Michael\\
%       \affaddr{Scientific Applications and Performance Tuning (SciAPT)},\\
%       \affaddr{               University Information Technology Services,}\\
       \affaddr{              Indiana University, Bloomington, IN, 47408}\\
       \email{scamicha@iu.edu}
% 3rd. author
\alignauthor 
Sen Xu\\
%       \affaddr{Scientific Applications and Performance Tuning (SciAPT)},\\
%       \affaddr{               University Information Technology Services,}\\
       \affaddr{              Indiana University, Bloomington, IN, 47408}\\
       \email{senxu@indiana.edu}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor 
Bernhard Haubold\\
%       \affaddr{Scientific Applications and Performance Tuning (SciAPT)},\\
%       \affaddr{               University Information Technology Services,}\\
       \affaddr{              Max-Planck Institute for Evolutionary Biology, Pl\"{o}n, Germany}\\
       \email{haubold@evolbio.mpg.de}
% 5th. author
\alignauthor 
Tom Doak\\
%       \affaddr{Scientific Applications and Performance Tuning (SciAPT)},\\
%       \affaddr{               University Information Technology Services,}\\
       \affaddr{              Indiana University, Bloomington, IN, 47408}\\
       \email{tdoak@indiana.edu}
% 6th. author
\alignauthor 
Robert Henschel\\
%       \affaddr{Scientific Applications and Performance Tuning (SciAPT)},\\
%       \affaddr{               University Information Technology Services,}\\
       \affaddr{              Indiana University, Bloomington, IN, 47408}\\
       \email{henschel@iu.edu}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Abstract ...
\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{PERFORMANCE, EXPERIMENTATION}

\keywords{mlRho, high-throughput, XSEDE, BigJob, pilot-job, genetics}

\section{Introduction}

As first defined by the National Science Foundation Advisory Committee for Cyberinfrastructure's task force on
campus bridging \cite{nsf2011}, and later expanded upon by \citeauthor{stewart2012}, campus bridging is:

\begin{quotation}
``...the seamlessly integrated use of cyberinfrastructure operated by a scientist or engineer
with other cyberinfrastructure on the scientist's campus, at other campuses, and at the regional, national,
and international levels as if they were proximate to the scientist, and when working within the context of a
Virtual Organization (VO) make the 'virtual' aspect of the organization irrelevant (or helpful) to the work of
the VO. \cite{stewart2012}''
\end{quotation}

In applying this definition of campus bridging to XSEDE, one of the biggest challenges for researchers moving
from campus resources to XSEDE resources is being able to scale-up their workflows so that they are efficient
and can achieve high throughput on the larger XSEDE machines. The operating environment is not very different
on XSEDE when compared to IU resources. In our situation, campus-bridging amounted to helping the researchers
re-design their experiments and scale-up their workflows to use the bigger machines effectively. In this paper
we discuss how we bridged the gap between the different approaches that are needed to succeed on machines of
different sizes.

The current incarnation of the mlRho software is a serial code. However, the investigation we have conducted
with our XSEDE allocation was embarrassingly parallel in nature, so the work for each species was able to be
divided among many computational processes. To manage hundreds of processes for each of the 40+ species of
interest, we used the BigJob pilot-job tool, which is currently available on many XSEDE resources. BigJob
allowed us to distribute the analyses for each species across hundreds to a few thousand processor cores
depending on the genome sizes of species. Thus far we have conducted ML recombination rate studies on a total
of {\bf XX} individual genomes. In addition to the use of the BigJob pilot tool we have done extensive performance
analysis of the mlRho code and, with biologist and computer scientists working in conjunction, have been able
to improve the runtime of the code by a factor of {\bf XX}. We have also begun work on a parallel
implementation of the code, and will present a comparison between the parallel implementation and the BigJob
framework.

The remainder of the paper is organized as follows: section \ref{sec:background} gives some insight into the
computational methodologies and principles of biology being explored by the mlRho code. Section
\ref{sec:resources} outlines our initial estimates for the computational resources necessary to accomplish our
research agenda and the steps that were taken to secure an XSEDE allocation. In section \ref{sec:optimization}
we describe how through tracing and profiling of the code we were able to successfully optimize it and
dramatically increase its performance. Section \ref{sec:results} details the current status of the research
program and some initial science results and in section \ref{sec:conclusion} we present some conclusions.


\section{Background}\label{sec:background}

Understanding how the amazing biodiversity on our planet has arisen is a biological question that has
fascinated humans for thousands of years. It is critical to investigate the fundamental ecological and genetic
parameters (e.g., population sizes, recombination rates) to disentangle the evolutionary process of the
existing organisms. The ecological and genetic parameters play important roles in creating opportunities for
increasing genetic diversity, population divergence, and speciation. Deploying mlRho, a software package which
generates a novel measure of linkage disequilibrium, on XSEDE resources has allowed us to study these
important population-genetic parameters in a broad assembly of eukaryotic species. This approach has been
particularly fruitful when applied to the newly available genome sequences now being produced by
next-generation sequencing methods.

Although there are several methods for determining recombination rates, they can be both time and resource
intensive. We have employed a novel analytic approach that employs a new metric called the zygosity
correlation coefficient estimated using maximum likelihood (ML). This approach requires single individual
samples, but is extremely data intensive. Using the ML approach implemented in the mlRho software developed at
IU and XSEDE computational resources, we have been able to examine the recombination rates of a plethora of
species with accuracy that was previously unachievable.

\subsection{Linkage Disequilibrium and Recombination Rate}\label{sec:LD}

Linkage disequilibrium (LD), i.e., the non-random association of alleles at two or more loci, is an important parameter for many areas of population genetics.  In recent years, there has been a growing interest in measuring LD across a broad range of species, because a proper understanding of LD would greatly facilitate identifying the genetic loci which underlie important phenotypic variation in natural populations, as well as human diseases. More importantly, LD is a population-genetic parameter that can help ascertain recombination rate, because recombination is the primary evolutionary force that breaks down LD among genetic loci. Although a substantial body of research has been devoted to determining the evolutionary advantages of recombination~\cite{resolving-paradox}, the forces that determine recombination rates remain poorly understood. For example, we have little idea what determines the occurrence of recombination on a chromosome, what impact local DNA polymorphism has on recombination processes, and how recombination rates change over evolutionary time~\cite{stumpf}. 

Conventional approaches to measuring LD and recombination rates use population-genetic surveys. These surveys require hundreds of individuals and dozens of genetic loci. Using this method, the sampling variance associated with conventional measures of LD, such as D (a measure of LD) and $r^2$ (the square of the correlation coefficient), is huge. Due to this fact, a large number of loci are needed to obtain a reasonable estimate of LD.  Recombination rates can also be investigated by pedigree analyses and crossing experiments, but these methods cannot provide information on fine-scale recombination rates and are often difficult to perform, even in model organisms, let alone non-model species. Thus, while we know these values for a few species, there is very little comparative data to understand how these processes vary across many species.

\subsection{ A Novel Approach to Estimating LD and Recombination Rate}\label{sec:migration}

Whole genome sequences of diploid organisms include both alleles at every site of the genome (two copies of each chromosome), which can be used to determine a number of very useful population-genetic parameters in the evolution a species. With the rapid accumulation of whole genome sequences from a large number of species, a maximum likelihood (ML) approach that capitalizes on these data has recently been developed in our lab to estimate LD and examine genomic recombination patterns \cite{Lynch01112008,MEC:MEC4482}.

The general idea behind this approach is that two allelic chromosomes had a common ancestor sometime in the past (just as any two humans share a common ancestor at some point in the past), and at that time, were identical. Since that time, they have become increasingly different, due both to mutations changing their sequence, and to each chromosome recombining with other chromosomes in the population (the larger the population, the more other chromosomes there are to recombine with). If we catalog the similarities and differences between the two alleles, we can learn about this history of mutation and recombination. 

The ML approach uses this information to determine the zygosity correlation ($\Delta$) between all the pairs of sites that are separated by various distances (i.e., the probability of two sites being both homozygous or both heterozygous) in a single diploid genome, using the entire set of individual reads. Population genetic theory then links the expected value of delta to conventional measures of LD, such as D and $r^2$, suggesting that $\Delta$ can be used as a valid measure of LD on the population level.

{\center \bf Insert equation 11, 14, and 5d from Lynch unpublished.}
\begin{eqnarray}
E(\Delta) \simeq {4E(D^2)\over \pi(1-\pi)}
\label{eq:edelta}
\end{eqnarray}

 \begin{eqnarray}
E(\Delta) = {{\theta(1+\theta)(18+\rho)[A+2-22\theta \rho(3+\rho+12\theta)]} \over {(10+\rho+8\theta)A}}r^2
\label{eq:edelta}
\end{eqnarray}

 \begin{eqnarray}
A = 9+6.5\rho + 0.5\rho^2+19\theta \rho+12\theta^2\rho+\theta \rho^2+54\theta+80\theta^2+32\theta^3
\label{eq:edelta}
\end{eqnarray}


It has become clear that delta is a function of the population mutation rate ($\theta$= 4 Ne $\mu$) and population recombination rate ($\rho$ = 4Nr), from which theta and rho can be estimated.

{\center \bf Insert equation 6 and equation 5d from Lynch unpublished.}


 \begin{eqnarray}
E(\Delta) \simeq {{\theta(1+2\theta)(18+\rho)} \over {2(1+\theta)A}}
\label{eq:edelta}
\end{eqnarray}

 \begin{eqnarray}
A = 9+6.5\rho + 0.5\rho^2+19\theta \rho+12\theta^2\rho+\theta \rho^2+54\theta+80\theta^2+32\theta^3
\label{eq:edelta}
\end{eqnarray}

Thus, when rho is zero, $\Delta$ is approximated by $\theta$; when $\rho$ is infinite, $\Delta$ is approximated by u/r, a parameter that reflects the relative power of mutation versus recombination. These two parameters are critical for evaluating the mutational-hazard hypothesis for genome evolution (Lynch 2007)

Our simulation results have shown that this ML procedure generates unbiased estimates for theta and zygosity-correlation at a range of sequencing coverage (10-20x) \cite{Lynch01112008}. 
This ML technique has been fully implemented in the software package mlRho. mlRho is a highly versatile program, able to handle genomes of various sizes, and linkage analyses over hundreds of thousands of base pairs. The maximum-likelihood estimates for $\Delta$ and recombination rates are efficiently sought using the Nelder and Mead algorithm, as implemented by the GNU scientific library. Nonetheless, this analysis is computationally extremely intensive: millions to tens of billions of pairs of sites at a given distance have to be extracted from the input files (usually of giga-byte sizes) and maximum likelihood estimates of $\Delta$ have to be calculated for every distance. 

%%%%

With the rapid accumulation of whole genome sequences from a large number of species, a ML approach that capitalizes on these data has recently been developed in our lab to estimate LD and examine genomic recombination patterns~\cite{Lynch01112008,MEC:MEC4482}. Whole genome sequences of diploid organisms include both alleles at every site of the genome, and the ML approach uses this information to determine the zygosity correlation (delta) between all the pairs of sites that are at various distances (i.e., the probability of two sites being both homozygous or both heterozygous) in a single diploid genome, using the entire set of individual reads. Population genetic theory then links the expected value of delta to conventional measures of LD, D and $r^2$ (Lynch, unpublished). 
%
%%{\center \bf Insert equation 11, 14, and 5d from Lynch unpublished.}
%\begin{eqnarray}
%E(\Delta) \simeq {4E(D^2)\over \pi(1-\pi)}
%\label{eq:edelta}
%\end{eqnarray}
%
%  \begin{eqnarray}
%E(\Delta) = {{\theta(1+\theta)(18+\rho)[A+2-22\theta \rho(3+\rho+12\theta)]} \over {(10+\rho+8\theta)A}}r^2
%\label{eq:edelta}
%\end{eqnarray}
%
%  \begin{eqnarray}
%A = 9+6.5\rho + 0.5\rho^2+19\theta \rho+12\theta^2\rho+\theta \rho^2+54\theta+80\theta^2+32\theta^3
%\label{eq:edelta}
%\end{eqnarray}
%
%
%It has become clear that delta is a function of the population mutation rate (theta=4Nu) and population recombination rate ($\rho=4Nr$). 
%
%%{\center \bf Insert equation 6 and equation 5d from Lynch unpublished.}
%
%
%  \begin{eqnarray}
%E(\Delta) \simeq {{\theta(1+2\theta)(18+\rho)} \over {2(1+\theta)A}}
%\label{eq:edelta}
%\end{eqnarray}
%
%  \begin{eqnarray}
%A = 9+6.5\rho + 0.5\rho^2+19\theta \rho+12\theta^2\rho+\theta \rho^2+54\theta+80\theta^2+32\theta^3
%\label{eq:edelta}
%\end{eqnarray}
%
%\abhi{explain equations?} \\
%\abhi{get figure 1}
%
%
%Thus, when rho is zero, delta is approximated by theta; when rho is infinite, delta is approximated by u/r, a parameter that reflects the relative power of mutation versus recombination. These two parameters are critical for evaluating the mutational-hazard hypothesis for genome evolution~\cite{lynch-origins}.
%
%Previous simulation results have shown that this ML procedure generates unbiased estimates for theta and zygosity-correlation at a sequencing coverage of 18-20x {\bf(Figure 1)}. More importantly, a correction technique that can eliminate the bias for these parameters at low coverage (e.g., 8x coverage, the typical coverage of most available genomes) has been developed in our lab {\bf (Figure 1)}. 
%
%{\center \bf Insert figures here.}
%
%
%This ML technique has been fully implemented in the software package mlRho. mlRho is a highly versatile program, able to handle genomes of various sizes, and linkage analyses over hundreds of thousands of base pairs. The maximum-likelihood estimates for theta, delta, and rho are efficiently sought using the Nelder and Mead algorithm, as implemented by the GNU scientific library. Nonetheless, this analysis is computationally extremely intensive: millions to tens of billions of pairs of sites at a given distance have to be extracted from the input files (usually of giga-byte sizes) and maximum likelihood estimates of delta have to be calculated for every distance. 
%
%%%%

\section{Deploying mlRho on XSEDE \\Resources}\label{sec:resources}
We have been using Quarry, a local IU resource before we decided to use XSEDE resources. Quarry is a 2960 core machine, where as Ranger is 	a 62,976 core machine, Kraken is a 112,896 core machine and Stampede is a 102400 core machine. \abhi{comment to self, consider adding a table}  The environment on Quarry is similar to that of XSEDE machines referred to here and the way users access it are also similar. The central difference is the size of the machines and therefore the biggest challenge is scaling our workflow accordingly. We explain how we made the transition and discuss the tools that we used to succeed on XSEDE.

\subsection{Transition to XSEDE}
\abhi{notes: startup alloc, scalability tests, etc}
We were faced with multiple questions at the start. Which of the large XSEDE machines suits us the best? Should we use more than one machine? How many SUs do we need to complete our research? Should we do everything we can or compress the amount of work we want to do depending on the number of SUs we can get from XSEDE?

We started by requesting startup allocations on Ranger and Kraken.  \abhi{cite something?} While it is possible to get good throughput submitting serial jobs to the scheduler on Quarry, this is not an optimal approach on machines like Ranger and Kraken. The scheduling policies on large machines usually favor large jobs. Many supercomputing centers have policies that limit the number of jobs a user can have in the queue. 


\subsubsection{Research Plan}\label{sec:plan}

\abhi{change the following paragraph to indicate what we actually did and why. i.e. doing more kbp/genome and may be more genomes}
We plan to use mlRho to estimate the LD and recombination patterns out to 100 kilo base-pair (kbp) distances for 46 diploid eukaryotic genomes, covering a broad range of genome sizes.%, as shown in table~\ref{table:genome_list}. 
The 100 kbp cutoff value is chosen because theory suggests that a good measure
of LD in a genome should span at least 50$-$100 kbp. Given that in eukaryotes, recombination rates range
between 0.001$-$1 event per Mbp~\cite{annurev-genom-082410-101412}, a 100 kbp window across a genome provides
a good basis for capturing the signature of crossover events that occur 1 or 2 times per chromosome arm per
meiotic event.

Our approach to linkage analyses requires diploid genomes that have not been intentionally inbred  (e.g., some plant genomes) or sequenced from several individuals. The list of 46 species are carefully selected from several important taxonomic groups to represent organisms with different ecology, population history, and reproductive strategies  (e.g., asexual vs. sexual). This list  thus constitutes a core dataset to develop a broad survey of linkage disequilibrium, population recombination rate, and population histories across eukaryotes. Furthermore, we have included genomes from multiple individuals for both {\it Daphnia pulex} and humans to explore intraspecific differences in linkage patterns and population histories. 

\subsubsection{Design of Experiments}
We have carefully constructed a detailed plan on how to proceed with our experiments. In section \ref{sec:tests}, we have shown that we can scale to 128 processes reading from the same file with no performance degradation. We estimate that we can feasibly scale-up to $\approx$500 processes reading from the same data file, without noticeable performance degradation. By making multiple copies of the data file, we could scale-up to $\approx$ 5000 processes. Job requests of 5000 processes on Ranger and Kraken are appropriate and we have not experienced extremely long wait times with past BigJob experiments. If we succeed in implementing this design, we think we can complete our analysis within four to five months from the time of the allocation award. This proposed framework is already working on Ranger, and we are in the process of implementing it on Kraken. 

In our initial testing of the three sample genomes, we worked out a few issues that could have affected
scalability. One of those issues was data access and file striping. When we had scaled up to 128 cores we
noticed intermittent I/O issues. Since all the concurrent instances read from a single file, if that file is
singly striped in a Lustre file system it will introduce a large load on the Lustre servers. We have remedied
this issue by moving the data files to the scratch file system on Ranger and striping the data files 16
ways. While this only occurred with the larger (11 and 31 GB) data files, we are aware that this might limit
our scalability. In the future, if I/O becomes an issue for scalability, we plan to use multiple copies of the
same data file to prevent an excessive number of concurrent processes from reading from the same file at the same time.

\subsubsection{SAGA BigJob framework}
\label{sec:bigjob}

BigJob is pilot-job tool available on many XSEDE resources such as Kraken, Ranger and Lonestar. Many researchers have successfully used BigJob~\cite{bigjob_web} to bundle hundreds of smaller jobs into larger, more manageable groups of jobs~\cite{Luckow:2008fp, async_repex11}. 

BigJob allows the user to maintain a list of processors allocated after the job request becomes active. The user can then assign these processors and start and manage jobs. For more details on the BigJob implementation see reference~\cite{saga_bigjob_condor_cloud}. The main benefit of doing this is that instead of submitting thousands of single core job requests to the queue, we can submit hundreds of large job requests ($\approx$ 500 to 5000 cores) to the queue. This reduces the overall job submissions to the queue and thereby, to an extent, time spent waiting in the queue. This job size is also more appropriate for many of the XSEDE machines. 

We plan to use BigJob to bundle our single-core serial mlRho simulations. We have integrated mlRho with BigJob and conducted several experiments on Ranger which have shown good performance and scalability. The experiments are explained in the next section. 


\subsubsection{mlRho Scalability Tests using BigJob on Ranger }
\label{sec:tests}


While each mlRho simulation is a serial job, we bundled many mlRho simulations using BigJob on Ranger. 
We used the BigJob installation already in place on Ranger. The major focus of our tests was to find out how mlRho scales when we increase the number of concurrent mlRho processes. 

We took three different organisms based on the size of their genome. We selected three sizes -- small (0.72 GB), medium (11 GB) and large (31 GB). We ran the mlRho program with each of these genomes. We started with 16 concurrent mlRho instances which are reading from the same data file, and let it run for 24 hours. We repeated this for each of these genomes with 16, 32, 64 and 128 concurrent instances. The results are shown in table~\ref{table:bj_runs} and figure~\ref{fig:scaling}.
Figure~\ref{fig:scaling} shows that the distance traveled, irrespective of the genome, size and the number of concurrent instances, increases linearly with number of concurrent instances being run. Table~\ref{table:bj_runs} shows the distance traveled by each instance in 24 hours for different genomes. This also shows that the distance traveled is a function of the size of the genome data file. As the reader can observe, distance traveled is directly proportional to the size of the genome data file. We were able to estimate the SUs required per distance per GB of genome data file using each of the three genomes -- all three estimates are nearly identical. Based on these results, we are confident that we can scale up to more than 1000 concurrent instances per genome using BigJob.




\section{Optimization and Performance Tuning of mlRho}\label{sec:optimization}
\subsection{Performance Analysis}\label{subsec:analysis}
%\subsection{Source-code Development and Optimization}
The mlRho application was developed through a collaboration of the Michael Lynch lab (at IU) and the Bernhard
Haubold lab (at the Max Planck Institute for Evolutionary Biology, Germany). This application is an implementation of
the novel ML approach. Researchers within the Research Technologies division of IU have been working together
with the mlRho application developers at IU and the Max Planck Institute for Evolutionary Biology to improve
the serial performance of the code. Since the ML approach is embarrassingly parallel, and the BigJob tool can
effectively distribute the workload to thousands of cores, research into parallelism for the code is not a priority right now. As a result of the initial investigations within Research Technologies, a new version has been recently released. 

The preliminary studies of Research Technologies has shown that further investigation into the serial
performance of the mlRho code is warranted. Particularly in the areas of I/O, memory usage, and call stack
depth. Research Technologies is planning to do a complete performance analysis of the code using Vampir to
find and remedy any of these inefficiencies in the code. 

\section{Experiments on XSEDE}{\label{sec:experiments}


\section{Findings and Preliminary Science Results}\label{sec:results}

\section{Conclusions}\label{sec:conclusion}



%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrvnat}
\bibliography{xsede13}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
\end{document}
